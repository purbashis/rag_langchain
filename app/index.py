"""
PDF ‚Üí Chunk ‚Üí Local Embedding ‚Üí Pinecone

This script:
1. Loads a PDF
2. Splits it into chunks
3. Creates local embeddings
4. Stores them inside Pinecone
"""

# ==============================
# üì¶ Imports (Dependencies)
# ==============================

import hashlib  # Generate unique IDs for chunks
import os  # File and path operations
from typing import List  # Type hints for functions

from dotenv import load_dotenv  # Load API keys from .env file
from langchain_community.document_loaders import PyPDFLoader  # Load PDF files
from langchain_text_splitters import RecursiveCharacterTextSplitter  # Split PDF text into chunks
from pinecone import Pinecone  # Vector database client for cloud storage
from sentence_transformers import SentenceTransformer  # Create text embeddings locally

# ==============================
# ‚öôÔ∏è Environment Setup
# ==============================

# Determine base directory and load .env file for API keys
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
load_dotenv(os.path.join(BASE_DIR, ".env"))  # Load API keys from .env


# ==============================
# üß† Local Embedding Model
# ==============================

# Load small fast embedding model (384-dimension output)
model = SentenceTransformer("all-MiniLM-L6-v2")


# ==============================
# üîë Utility Functions
# ==============================

# Generate unique ID for each chunk (prevents duplicates)
def generate_id(text: str, index: int) -> str:
    return hashlib.md5((text + str(index)).encode("utf-8")).hexdigest()


# Convert list of texts ‚Üí embedding vectors
def embed_texts(texts: List[str]):
    return model.encode(texts).tolist()


# ==============================
# üöÄ Main Indexing Logic
# ==============================

# Main function to load PDF, chunk, embed, and store in Pinecone
def index_document():

    print("üöÄ Starting indexing process...\n")

    # 1Ô∏è‚É£ Load PDF file
    pdf_path = os.path.join(BASE_DIR, "data", "dsa.pdf")
    loader = PyPDFLoader(pdf_path)
    raw_docs = loader.load()
    print("‚úÖ PDF loaded")

    # 2Ô∏è‚É£ Split long text into smaller chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,     # size of each chunk
        chunk_overlap=200    # overlapping context
    )
    chunks = splitter.split_documents(raw_docs)
    print(f"‚úÖ Chunking completed ({len(chunks)} chunks)")

    # Extract pure text from chunks
    texts = [chunk.page_content for chunk in chunks]

    # 3Ô∏è‚É£ Generate embeddings locally
    print("üîÑ Generating local embeddings...")
    vectors = embed_texts(texts)
    print("‚úÖ Embeddings created")

    # 4Ô∏è‚É£ Connect to Pinecone
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    pinecone_index = pc.Index(os.getenv("PINECONE_INDEX_NAME"))

    # 5Ô∏è‚É£ Prepare data for upsert
    upsert_payload = []

    # Create payload with unique ID, embedding vector, and metadata (original text)
    for i, vector in enumerate(vectors):
        text = texts[i]

        upsert_payload.append({
            "id": generate_id(text, i),   # unique ID
            "values": vector,             # embedding vector
            "metadata": {                 # store text for retrieval
                "text": text[:1500],
                "source": "dsa.pdf"
            }
        })

    # 6Ô∏è‚É£ Store vectors in Pinecone
    pinecone_index.upsert(vectors=upsert_payload)

    print("\nüéâ Data stored successfully!")


# ==============================
# ‚ñ∂Ô∏è Script Entry Point
# ==============================

if __name__ == "__main__":
    index_document()


# ============================================================
# üîÆ FUTURE: If You Want To Use Google Embeddings Again
# ============================================================

"""
Replace local embedding part with:

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="embedding-001",
    google_api_key=os.getenv("GEMINI_API_KEY")
)

vectors = embeddings.embed_documents(texts)

‚ö†Ô∏è Make sure:
- You enable billing
- Avoid quota limits
"""
