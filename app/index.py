"""
PDF â†’ Chunk â†’ Local Embedding â†’ Pinecone

This script:
1. Loads a PDF
2. Splits it into chunks
3. Creates local embeddings
4. Stores them inside Pinecone
"""

# ==============================
# ğŸ“¦ Imports (Dependencies)
# ==============================

import os
import hashlib
from typing import List

from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer


# ==============================
# âš™ï¸ Environment Setup
# ==============================

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
load_dotenv(os.path.join(BASE_DIR, ".env"))  # Load API keys from .env


# ==============================
# ğŸ§  Local Embedding Model
# ==============================

# Load small fast embedding model (384-dimension output)
model = SentenceTransformer("all-MiniLM-L6-v2")


# ==============================
# ğŸ”‘ Utility Functions
# ==============================

# Generate unique ID for each chunk (prevents duplicates)
def generate_id(text: str, index: int) -> str:
    return hashlib.md5((text + str(index)).encode("utf-8")).hexdigest()


# Convert list of texts â†’ embedding vectors
def embed_texts(texts: List[str]):
    return model.encode(texts).tolist()


# ==============================
# ğŸš€ Main Indexing Logic
# ==============================

def index_document():

    print("ğŸš€ Starting indexing process...\n")

    # 1ï¸âƒ£ Load PDF file
    pdf_path = os.path.join(BASE_DIR, "data", "dsa.pdf")
    loader = PyPDFLoader(pdf_path)
    raw_docs = loader.load()
    print("âœ… PDF loaded")

    # 2ï¸âƒ£ Split long text into smaller chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,     # size of each chunk
        chunk_overlap=200    # overlapping context
    )
    chunks = splitter.split_documents(raw_docs)
    print(f"âœ… Chunking completed ({len(chunks)} chunks)")

    # Extract pure text from chunks
    texts = [chunk.page_content for chunk in chunks]

    # 3ï¸âƒ£ Generate embeddings locally
    print("ğŸ”„ Generating local embeddings...")
    vectors = embed_texts(texts)
    print("âœ… Embeddings created")

    # 4ï¸âƒ£ Connect to Pinecone
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    pinecone_index = pc.Index(os.getenv("PINECONE_INDEX_NAME"))

    # 5ï¸âƒ£ Prepare data for upsert
    upsert_payload = []

    for i, vector in enumerate(vectors):
        text = texts[i]

        upsert_payload.append({
            "id": generate_id(text, i),   # unique ID
            "values": vector,             # embedding vector
            "metadata": {                 # store text for retrieval
                "text": text[:1500],
                "source": "dsa.pdf"
            }
        })

    # 6ï¸âƒ£ Store vectors in Pinecone
    pinecone_index.upsert(vectors=upsert_payload)

    print("\nğŸ‰ Data stored successfully!")


# ==============================
# â–¶ï¸ Script Entry Point
# ==============================

if __name__ == "__main__":
    index_document()


# ============================================================
# ğŸ”® FUTURE: If You Want To Use Google Embeddings Again
# ============================================================

"""
Replace local embedding part with:

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="embedding-001",
    google_api_key=os.getenv("GEMINI_API_KEY")
)

vectors = embeddings.embed_documents(texts)

âš ï¸ Make sure:
- You enable billing
- Avoid quota limits
"""
